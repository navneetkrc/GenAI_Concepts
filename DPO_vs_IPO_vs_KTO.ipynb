{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg1s8RfKIHr9jTRoAt6HZZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Comparison of DPO vs IPO vs KTO"
      ],
      "metadata": {
        "id": "iefp4wwhHjGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preference Tuning LLMs: Visual Summary\n",
        "\n",
        "## Overview of the Blog\n",
        "This blog post compares three direct preference optimization methods for aligning language models without reinforcement learning:\n",
        "- **Direct Preference Optimization (DPO)**\n",
        "- **Identity Preference Optimization (IPO)**\n",
        "- **Kahneman-Tversky Optimization (KTO)**\n",
        "\n",
        "The researchers performed experiments across different hyperparameter settings to evaluate which method produces the best chat models.\n",
        "\n",
        "## Experimental Setup Visual Flow\n",
        "```\n",
        "┌───────────────────┐     ┌───────────────────┐\n",
        "│ Base Models       │     │ Datasets          │\n",
        "│ ───────────────── │     │ ───────────────── │\n",
        "│ • OpenHermes-2.5  │     │ • orca_dpo_pairs  │\n",
        "│   Mistral-7B      │     │   (13k prompts)   │\n",
        "│ • Zephyr-7b-beta- │     │ • ultrafeedback-  │\n",
        "│   sft             │     │   binarized (66k) │\n",
        "└─────────┬─────────┘     └────────┬──────────┘\n",
        "          │                        │\n",
        "          └──────────┬─────────────┘\n",
        "                     ▼\n",
        "┌──────────────────────────────────────────────┐\n",
        "│ Hyperparameter Configurations                │\n",
        "│ ────────────────────────────────────────     │\n",
        "│ • Methods: DPO, IPO, KTO                     │\n",
        "│ • β values: 0.01, 0.1, 0.2, ..., 0.9         │\n",
        "│ • One epoch training                         │\n",
        "└──────────────────────┬───────────────────────┘\n",
        "                       ▼\n",
        "┌──────────────────────────────────────────────┐\n",
        "│ Evaluation                                   │\n",
        "│ ───────────                                  │\n",
        "│ • MT-Bench (GPT-4 judges performance)        │\n",
        "│ • Categories: Writing, Roleplay, Reasoning,  │\n",
        "│   Math, Coding, Extraction, STEM, Humanities │\n",
        "└──────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "## Key Findings Visualized\n",
        "\n",
        "### Zephyr Model Results\n",
        "```\n",
        "  MT-Bench\n",
        "  Score\n",
        "    ↑\n",
        "6.0 │                                   ●\n",
        "    │           ●                      \n",
        "5.5 │          ▲          ●           ▲\n",
        "    │                     ▲\n",
        "5.0 │ ▲                              \n",
        "    │                               \n",
        "4.5 │                              \n",
        "    │                              \n",
        "4.0 └───────────────────────────────────→ β value\n",
        "      0.01  0.1   0.2   0.3  ...   0.9\n",
        "\n",
        "● DPO   ▲ KTO   ■ IPO   --- Base model\n",
        "```\n",
        "\n",
        "### OpenHermes Model Results\n",
        "```\n",
        "  MT-Bench\n",
        "  Score\n",
        "    ↑\n",
        "8.0 │                      ●\n",
        "    │                     \n",
        "7.5 │        ▲                         \n",
        "    │                     \n",
        "7.0 │ ■                              \n",
        "    │                               \n",
        "6.5 │                              \n",
        "    │                              \n",
        "6.0 └───────────────────────────────────→ β value\n",
        "      0.01  0.1   0.2   0.3  ...   0.9\n",
        "\n",
        "● DPO   ▲ KTO   ■ IPO   --- Base model\n",
        "```\n",
        "\n",
        "## Conclusions\n",
        "- DPO consistently achieved the best performance\n",
        "- The optimal β value varies across methods and models\n",
        "- For Zephyr, lower β values (0.01) worked best across all methods\n",
        "- For OpenHermes, optimal β values varied (DPO: 0.6, KTO: 0.3, IPO: 0.01)\n",
        "- DPO > KTO > IPO in performance ranking"
      ],
      "metadata": {
        "id": "AiXYnyagzaY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll create a visual explanation of this blog post about preference tuning LLMs, along with a comparison table of the three approaches.\n",
        "\n",
        "\n",
        "\n",
        "# Preference Tuning LLMs: Visual Summary\n",
        "\n",
        "## Overview of the Blog\n",
        "This blog post compares three direct preference optimization methods for aligning language models without reinforcement learning:\n",
        "- **Direct Preference Optimization (DPO)**\n",
        "- **Identity Preference Optimization (IPO)**\n",
        "- **Kahneman-Tversky Optimization (KTO)**\n",
        "\n",
        "The researchers performed experiments across different hyperparameter settings to evaluate which method produces the best chat models.\n",
        "\n",
        "## Experimental Setup Visual Flow\n",
        "```\n",
        "┌───────────────────┐     ┌───────────────────┐\n",
        "│ Base Models       │     │ Datasets          │\n",
        "│ ───────────────── │     │ ───────────────── │\n",
        "│ • OpenHermes-2.5  │     │ • orca_dpo_pairs  │\n",
        "│   Mistral-7B      │     │   (13k prompts)   │\n",
        "│ • Zephyr-7b-beta- │     │ • ultrafeedback-  │\n",
        "│   sft             │     │   binarized (66k) │\n",
        "└─────────┬─────────┘     └────────┬──────────┘\n",
        "          │                        │\n",
        "          └──────────┬─────────────┘\n",
        "                     ▼\n",
        "┌──────────────────────────────────────────────┐\n",
        "│ Hyperparameter Configurations                │\n",
        "│ ────────────────────────────────────────     │\n",
        "│ • Methods: DPO, IPO, KTO                     │\n",
        "│ • β values: 0.01, 0.1, 0.2, ..., 0.9         │\n",
        "│ • One epoch training                         │\n",
        "└──────────────────────┬───────────────────────┘\n",
        "                       ▼\n",
        "┌──────────────────────────────────────────────┐\n",
        "│ Evaluation                                   │\n",
        "│ ───────────                                  │\n",
        "│ • MT-Bench (GPT-4 judges performance)        │\n",
        "│ • Categories: Writing, Roleplay, Reasoning,  │\n",
        "│   Math, Coding, Extraction, STEM, Humanities │\n",
        "└──────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "## Key Findings Visualized\n",
        "\n",
        "### Zephyr Model Results\n",
        "```\n",
        "  MT-Bench\n",
        "  Score\n",
        "    ↑\n",
        "6.0 │                                   ●\n",
        "    │           ●                      \n",
        "5.5 │          ▲          ●           ▲\n",
        "    │                     ▲\n",
        "5.0 │ ▲                              \n",
        "    │                               \n",
        "4.5 │                              \n",
        "    │                              \n",
        "4.0 └───────────────────────────────────→ β value\n",
        "      0.01  0.1   0.2   0.3  ...   0.9\n",
        "\n",
        "● DPO   ▲ KTO   ■ IPO   --- Base model\n",
        "```\n",
        "\n",
        "### OpenHermes Model Results\n",
        "```\n",
        "  MT-Bench\n",
        "  Score\n",
        "    ↑\n",
        "8.0 │                      ●\n",
        "    │                     \n",
        "7.5 │        ▲                         \n",
        "    │                     \n",
        "7.0 │ ■                              \n",
        "    │                               \n",
        "6.5 │                              \n",
        "    │                              \n",
        "6.0 └───────────────────────────────────→ β value\n",
        "      0.01  0.1   0.2   0.3  ...   0.9\n",
        "\n",
        "● DPO   ▲ KTO   ■ IPO   --- Base model\n",
        "```\n",
        "\n",
        "## Conclusions\n",
        "- DPO consistently achieved the best performance\n",
        "- The optimal β value varies across methods and models\n",
        "- For Zephyr, lower β values (0.01) worked best across all methods\n",
        "- For OpenHermes, optimal β values varied (DPO: 0.6, KTO: 0.3, IPO: 0.01)\n",
        "- DPO > KTO > IPO in performance ranking\n",
        "\n",
        "\n",
        "```markdown\n",
        "# Comparison of Direct Preference Optimization Methods\n",
        "\n",
        "| Feature | Direct Preference Optimization (DPO) | Identity Preference Optimization (IPO) | Kahneman-Tversky Optimization (KTO) |\n",
        "|---------|--------------------------------------|---------------------------------------|-------------------------------------|\n",
        "| **Core Approach** | Recasts alignment as a simple loss function optimized on preference datasets | Adds regularization to DPO loss to prevent overfitting | Uses individual examples labeled as \"good\" or \"bad\" instead of paired preferences |\n",
        "| **Data Requirements** | Requires paired preferences (chosen vs. rejected responses) | Requires paired preferences (chosen vs. rejected responses) | Can work with unpaired data (responses marked good/bad) |\n",
        "| **Key Advantage** | Simpler to implement than RLHF; strong empirical results | Better theoretical guarantees; can train to convergence without early stopping | Can use simpler data (e.g., thumbs up/down feedback) |\n",
        "| **Best β Value (OpenHermes)** | 0.6 | 0.01 | 0.3 |\n",
        "| **Best β Value (Zephyr)** | 0.01 | 0.01 | 0.01 |\n",
        "| **Best MT-Bench Score (OpenHermes)** | 7.57 (β=0.6) | 6.99 (β=0.01) | 7.33 (β=0.3) |\n",
        "| **Best MT-Bench Score (Zephyr)** | 5.92 (β=0.1) | 5.39 (β=0.01) | 5.63 (β=0.9) |\n",
        "| **Performance Ranking** | 1st | 3rd | 2nd |\n",
        "| **Ideal Use Case** | General purpose alignment | When overfitting is a concern | Production systems with user feedback |\n",
        "| **Implementation Complexity** | Low | Medium | Low |\n",
        "| **Limitations** | Can overfit quickly to preference dataset | More complex formulation | May underperform compared to DPO with paired data |\n",
        "\n",
        "```\n",
        "\n",
        "The blog post you shared compares three different methods for aligning language models (LLMs) without using reinforcement learning. Here's a summary of the key points:\n",
        "\n",
        "1. **The Methods**: The research compares Direct Preference Optimization (DPO), Identity Preference Optimization (IPO), and Kahneman-Tversky Optimization (KTO).\n",
        "\n",
        "2. **Experimental Setup**:\n",
        "   - They tested on two 7B parameter models: OpenHermes-2.5-Mistral-7B and Zephyr-7b-beta-sft\n",
        "   - Used two datasets: Intel's orca_dpo_pairs (13k prompts) and ultrafeedback-binarized (66k prompts)\n",
        "   - Tested multiple β values (0.01 to 0.9) which control how much to weight the preference of the reference model\n",
        "\n",
        "3. **Key Findings**:\n",
        "   - DPO consistently performed best, followed by KTO, with IPO performing worst\n",
        "   - The optimal β value varied significantly between models and methods\n",
        "   - For Zephyr, lower β values (0.01) worked best across all methods\n",
        "   - For OpenHermes, optimal β values were DPO: 0.6, KTO: 0.3, IPO: 0.01\n",
        "\n",
        "4. **Practical Applications**:\n",
        "   - DPO requires paired preference data but gives the best results\n",
        "   - KTO is promising because it can work with simpler feedback (like thumbs up/down) rather than paired preferences\n",
        "   - IPO offers theoretical benefits for preventing overfitting but didn't perform as well in practice\n",
        "\n",
        "The artifacts provide a visual explanation of the blog content and a detailed comparison table of the three methods, showing their key differences, optimal parameters, and performance metrics."
      ],
      "metadata": {
        "id": "j6lz15yL0Dy4"
      }
    }
  ]
}