# üîù Top 25 NLP Interview Questions

---

## üîπ Core Concepts

### ‚úÖ What is Natural Language Processing (NLP)??

NLP is a field of AI that enables machines to understand, interpret, generate, and interact using human language.
It combines **linguistics** and **machine learning** to process text or speech data.

---

### ‚úÖ What is the difference between NLP and NLU (Natural Language Understanding)??

| **Aspect**  | **NLP**                              | **NLU**                              |
| ----------- | ------------------------------------ | ------------------------------------ |
| **Goal**    | General text processing              | Understanding meaning & intent       |
| **Scope**   | Includes NLU, NLG, and preprocessing | Subset of NLP                        |
| **Example** | Tokenization, POS tagging            | Intent detection, sentiment analysis |

**üîÅ Note:** NLU focuses on meaning and semantics within NLP.

---

### ‚úÖ What is tokenization and why is it important?

Tokenization splits text into smaller units called **tokens** (words, subwords, characters).
It's the **first step** in most NLP pipelines, making raw text processable for models.

---

### ‚úÖ What are stop words, and should we always remove them?

Stop words are common words like *"the"*, *"is"*, *"in"* that often carry little semantic value.
**Should we remove them?**

‚úî Often removed to reduce noise and improve efficiency.
‚ö† But retained for tasks like translation or question answering where every word matters.

---

### ‚úÖ Explain stemming vs lemmatization with examples.

| **Feature** | **Stemming**          | **Lemmatization**      |
| ----------- | --------------------- | ---------------------- |
| **Method**  | Rule-based chopping   | Dictionary + POS-aware |
| **Output**  | Not always real words | Real base words        |
| **Example** | `"running"` ‚Üí `runn`  | `"running"` ‚Üí `run`    |

üß† *Stemming is faster but less accurate; lemmatization gives better linguistic correctness.*

---

## üîπ Embeddings & Representations

### ‚úÖ What are word embeddings? How do they work?

**Short Answer:**
Word embeddings are dense vector representations capturing word meanings via context and relationships.

**Detailed Explanation:**
They map words to fixed-size vectors where similar meanings have similar vectors.
Learned from large corpora by models like **Word2Vec** or **GloVe**, they capture semantic relationships beyond simple one-hot encoding.

---

### ‚úÖ How do Word2Vec, GloVe, and FastText differ?

| **Model**    | **Key Idea**                                                               |
| ------------ | -------------------------------------------------------------------------- |
| **Word2Vec** | Predicts words using local context (Skip-gram/CBOW)                        |
| **GloVe**    | Builds word vectors from global co-occurrence statistics                   |
| **FastText** | Uses subword n-grams to represent words, handling rare/unseen words better |

**Infographic Suggestion:**
Illustrate three pipelines showing Word2Vec (local window), GloVe (corpus-wide matrix), and FastText (subword composition).

---

### ‚úÖ What is the Out-of-Vocabulary (OOV) problem and how can it be handled?

**OOV:** When a word wasn't seen during training, making it impossible to generate its embedding.

**Solutions:**
‚úî FastText's subword n-grams compose unseen words.
‚úî Contextual models (e.g., **BERT**) generate dynamic embeddings based on sentence context.
‚úî `<UNK>` token for unknown words in simpler models.

---

### ‚úÖ What are sentence and document embeddings? How do they differ from word embeddings?

| **Level**             | **Representation**                                   |
| --------------------- | ---------------------------------------------------- |
| **Word**              | Individual word vectors                              |
| **Sentence/Document** | Vector summarizing full sentence or document meaning |

**Methods:**
‚úî Averaging word embeddings (basic)
‚úî Pretrained models like **Sentence-BERT**, **Universal Sentence Encoder**

**Use Cases:** Classification, Semantic Search, Clustering.

---

### ‚úÖ Explain static vs contextual word embeddings.

| **Aspect**     | **Static**                | **Contextual**                          |
| -------------- | ------------------------- | --------------------------------------- |
| **Definition** | One fixed vector per word | Vectors vary based on sentence context  |
| **Examples**   | Word2Vec, GloVe           | BERT, RoBERTa                           |
| **Limitation** | Cannot resolve ambiguity  | Handles polysemy, captures true meaning |

---

## üîπ Attention & Transformers

### ‚úÖ What is the attention mechanism? How does it work?

**Short Answer:**
Attention allows models to focus on relevant input parts during predictions.

**Detailed Steps:**

1. Convert tokens to **Queries (Q)**, **Keys (K)**, **Values (V)**

2. Compute attention weights:

   $$
   \text{Attention} = \text{softmax} \left( \frac{Q \cdot K^T}{\sqrt{d_k}} \right)
   $$

3. Output: Weighted sum of **V** emphasizing important tokens.

**Infographic Suggestion:** Visual showing token interactions with varying attention weights.

---

### ‚úÖ Self-attention vs Cross-attention

| **Type**            | **Description**                                         | **Use Case**                          |
| ------------------- | ------------------------------------------------------- | ------------------------------------- |
| **Self-attention**  | Tokens attend to others in the same sequence            | Captures intra-sequence relationships |
| **Cross-attention** | One sequence attends to another (e.g., encoder-decoder) | Translation, Summarization            |

---

### ‚úÖ What are Transformers and why are they important in NLP?

**Short Answer:**
Transformers use self-attention to model long-range dependencies efficiently.

**Key Features:**
‚úî Parallelizable
‚úî Capture complex dependencies
‚úî Backbone of models like **BERT**, **GPT**, **T5**

**Infographic Suggestion:** Block diagram showing encoder-decoder structure with attention layers.

---

### ‚úÖ Compare RNN, LSTM, GRU, Transformer models

| **Model**       | **Pros**                        | **Cons**                          |
| --------------- | ------------------------------- | --------------------------------- |
| **RNN**         | Simple, sequential processing   | Struggles with long dependencies  |
| **LSTM**        | Handles long-range dependencies | Slow, step-by-step                |
| **GRU**         | Faster than LSTM                | Still sequential, less expressive |
| **Transformer** | Parallel, long-range context    | Heavy for very long sequences     |

---

### ‚úÖ Local vs Global Attention

| **Type**   | **Scope**                | **Use Case**                      |
| ---------- | ------------------------ | --------------------------------- |
| **Local**  | Focuses on nearby tokens | Long documents (e.g., Longformer) |
| **Global** | Attends to all tokens    | Full-sequence understanding       |

**Tradeoff:**
Local attention improves efficiency; global captures complete relationships.

---

## üîπ Language Models

### ‚úÖ What is a language model? How is it trained?

Predicts sequence likelihood:

$$
P(w_1, w_2, ..., w_n)
$$

**Training:**
‚úî Next-word prediction (Causal LM, e.g., GPT)
‚úî Masked word prediction (MLM, e.g., BERT)

Forms the foundation for generation, search, translation, etc.

---

### ‚úÖ MLM vs CLM

| **Aspect**   | **MLM (Masked)**     | **CLM (Causal)**   |
| ------------ | -------------------- | ------------------ |
| **Goal**     | Predict masked words | Predict next word  |
| **Context**  | Uses left & right    | Only left-to-right |
| **Examples** | BERT, RoBERTa        | GPT, GPT-3         |

**Example:**
MLM: `"The cat sat on the [MASK]"` ‚Üí Predict `"mat"`
CLM: `"The cat sat on the"` ‚Üí Predict `"mat"`

---

### ‚úÖ How does BERT work? Why bidirectional?

BERT uses:
‚úî **Bidirectional self-attention**
‚úî **Masked Language Modeling (MLM)**
‚úî **Next Sentence Prediction (NSP)**

Processes entire sentences, capturing rich context for classification, QA, etc.

---

### ‚úÖ GPT vs BERT

| **Feature**      | **GPT**               | **BERT**               |
| ---------------- | --------------------- | ---------------------- |
| **Training**     | Causal LM (next word) | Masked LM + NSP        |
| **Architecture** | Decoder stack         | Encoder stack          |
| **Focus**        | Text generation       | Language understanding |
| **Context**      | Left-to-right         | Bidirectional          |

---

### ‚úÖ What is fine-tuning?

Adapting pretrained models to downstream tasks:

**Steps:**
‚úî Add task-specific layers (e.g., classifier, QA head)
‚úî Train on labeled task data
‚úî Benefits: Few labeled examples required, leverages pretrained knowledge

---

## üîπ Evaluation & Applications

### ‚úÖ What is BLEU score?

Measures machine translation quality via n-gram overlap:

$$
\text{BLEU} = \text{Brevity Penalty} \times \text{Geometric Mean of n-gram Precisions}
$$

**Range:** 0 to 1 (higher = better)
‚ö† Limitation: Focuses on surface overlap, may miss true semantic quality.

---

### ‚úÖ What is perplexity?

Quantifies model's uncertainty:

$$
\text{Perplexity} = 2^{-\text{average log probability of tokens}}
$$

Lower perplexity = better predictions (used in evaluating generative models).

---

### ‚úÖ NER and POS Tagging

| **Task**        | **Function**                        | **Example**                      |
| --------------- | ----------------------------------- | -------------------------------- |
| **NER**         | Detects entities (names, locations) | `"Barack Obama"` ‚Üí PERSON        |
| **POS Tagging** | Labels grammatical roles            | `"fox"` ‚Üí NOUN, `"jumps"` ‚Üí VERB |

Essential for syntactic and semantic understanding.

---

### ‚úÖ How does semantic search work using embeddings?

**Process:**
‚úî Convert queries/documents to embeddings (e.g., Sentence-BERT)
‚úî Compute similarity (cosine similarity)
‚úî Retrieve conceptually similar content, beyond keyword matches

**Example:**
Query: `"Symptoms of flu"`
Retrieves: `"How to treat viral fever"`

---

### ‚úÖ End-to-End Text Classification Pipeline

**Steps:**

1. Data Collection
2. Preprocessing: Cleaning, tokenization
3. Embedding: Word2Vec, Sentence-BERT, or Transformers
4. Model Training: Classifier or fine-tuned language model
5. Evaluation: Metrics like Accuracy, F1
6. Deployment: API exposure
7. Monitoring & Retraining

---

## üéØ **Infographics Recommendations**

* **Embedding Models Comparison:** Word2Vec vs GloVe vs FastText visual

 ![image](https://github.com/user-attachments/assets/f6f63692-a7e9-4cbb-b496-a0729e4563eb)

* **Attention Mechanism:** Query-Key-Value flow diagram
https://medium.com/data-science/what-are-query-key-and-value-in-the-transformer-architecture-and-why-are-they-used-acbe73f731f2

  ![image](https://github.com/user-attachments/assets/d8ba1980-3e79-424e-95aa-e4067c31fcae)
---
* **Transformer Block Diagram:** Encoder, Decoder, Attention layers
* refer https://medium.com/data-science-community-srm/understanding-encoders-decoders-with-attention-based-mechanism-c1eb7164c581
  
![image](https://github.com/user-attachments/assets/44fd90ed-a90e-4a98-91d8-9b6398c89790)

Classic Encoder Decoder -> observe so much of relevance of single connection between Encoder and Decoders

![image](https://github.com/user-attachments/assets/1d95f3bb-8862-4bee-9706-4ef8fef5d706)

See the improvement from regular Encoder-Decoder single point of failure

---
![image](https://github.com/user-attachments/assets/b0d42713-2853-48de-9312-0527f3029c49)
Encoder-Decoder with simple fixed context vector


![image](https://github.com/user-attachments/assets/734f6ee5-a917-44b5-96d9-771b1664ee1d)

Encoder-decoder with attention-based mechanism(https://zhuanlan.zhihu.com/p/37290775)




---
* **Local vs Global Attention:** Token connectivity illustration
* ![image](https://github.com/user-attachments/assets/9cf5e8bc-4ad9-4c86-b872-62efc79c52c8)

Attention matrices of different types of Attention mechanisms. Image sourced from https://medium.com/nlplanet/two-minutes-nlp-visualizing-global-vs-local-attention-c61b42758019

---
* **Semantic Search Workflow:** Query-to-Embedding to Result flow
* https://blog.ml6.eu/semantic-search-a-practical-overview-bf2515e7be76
---

### üîç **Lexical Search vs Semantic Search**

* **Lexical Search** relies on exact word matches between the query and documents. If the words don't match, even relevant results are missed.

  * *Example:*
    Query: ‚ÄúTwo bedroom house in Los Angeles‚Äù
    Misses: ‚ÄúA residence with 2 rooms in sunny California‚Äù (different wording)
    Finds: ‚ÄúA two-story house in Los Angeles‚Äù (shares words but different meaning)

* **Semantic Search** focuses on the meaning, not just words. It understands synonyms and related concepts, delivering results even with different phrasing.

  * *Example:*
    Understands ‚Äúhouse‚Äù ‚âà ‚Äúresidence‚Äù, ‚Äú2 rooms‚Äù ‚âà ‚Äútwo bedroom‚Äù, ‚ÄúCalifornia‚Äù ‚âà ‚ÄúLos Angeles‚Äù contextually
    Correctly ranks: ‚ÄúA residence with 2 rooms in sunny California‚Äù as relevant

**Bottom line:**
‚û°Ô∏è Lexical = Exact word match, may miss the point
‚û°Ô∏è Semantic = Meaning match, captures intent even with different words

---


* ![image](https://github.com/user-attachments/assets/9e93a0d2-8697-4cfc-b8f2-38384d5f9987)

  ---

* ![image](https://github.com/user-attachments/assets/8e3755b0-d643-4c5d-97d3-4b86e9420b8d)

* ![image](https://github.com/user-attachments/assets/a6ab72a6-d2aa-4d20-9860-294da7b25814)



---
