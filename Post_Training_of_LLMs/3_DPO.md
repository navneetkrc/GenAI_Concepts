# ğŸ“ Short Notes on DPO (Direct Preference Optimization)
---
<img width="1429" height="799" alt="Screenshot 2025-09-06 at 7 46 08â€¯PM" src="https://github.com/user-attachments/assets/356126e6-fc37-4401-882f-80bca345a3f8" />

**Core Idea:**

* DPO fine-tunes LLMs by learning from **comparison data** (positive vs negative responses).
* Instead of reward models (RLHF), it directly optimizes model likelihood to prefer human-labeled *better answers* over worse ones.
* Uses **contrastive learning**: pushes model toward the preferred response, away from the rejected one.

**Formula Intuition:**

* Loss compares modelâ€™s log-probability of positive vs negative responses, normalized against a reference model.
* Effect: More stable than RLHF, easier to train, avoids reward hacking.

---

# ğŸ­ Industry Use Cases

### ğŸ”¹ E-commerce Search (Samsung Example)

1. **Query Disambiguation**

   * *User query:* â€œSamsung foldâ€
   * âŒ Negative response: Maps only to â€œGalaxy Z Fold 2 (outdated)â€
   * âœ… Positive response: Suggests the latest **Galaxy Z Fold 6**, with context on upgrades.
   * ğŸ‘‰ DPO ensures search results prefer the **most relevant, updated SKU**.

2. **Answering Product Questions**

   * *Query:* â€œDoes this TV support Netflix?â€
   * âŒ Negative: â€œThis TV has apps.â€ (vague, unhelpful)
   * âœ… Positive: â€œYes, Samsung Neo QLED TVs support Netflix pre-installed.â€
   * ğŸ‘‰ DPO improves **answer helpfulness** in product Q\&A.

3. **Personalized Recommendations**

   * *Query:* â€œBest phone for studentsâ€
   * âŒ Negative: Suggests high-end **Galaxy S Ultra** (expensive, irrelevant)
   * âœ… Positive: Suggests **Galaxy A series** with priceâ€“performance balance.
   * ğŸ‘‰ DPO aligns results with **human preferences**, not just popularity.

---

# ğŸ¤ Interview Soundbite

ğŸ‘‰ *â€œDPO helps align Samsungâ€™s e-commerce search to **human preferences** â€” preferring answers that are accurate, helpful, and contextual. For example, when users search â€˜Samsung foldâ€™, DPO ensures the model suggests the **latest Galaxy Z Fold** instead of outdated models. This makes search more reliable and user-friendly compared to raw ranking models.â€*

---
---

# âš–ï¸ RLHF vs DPO in E-commerce Search

---

## ğŸ”¹ **Training Setup**

* **RLHF** ğŸŒ€: Multi-step â†’ SFT â†’ Reward Model â†’ PPO.
* **DPO** ğŸ¯: Single-step â†’ Directly optimize using preference pairs.

---

## ğŸ”¹ **Complexity & Cost**

* **RLHF** ğŸ’¸: Expensive, needs reward model + online sampling.
* **DPO** âš¡: Cheaper, stable, no reward model required.

---

## ğŸ”¹ **E-commerce Example (Samsung Search)**

* **RLHF**:
  ğŸ” Query: *â€œGalaxy Foldâ€* â†’ model overfits to **older Z Fold 2** (higher historical clicks).
* **DPO**:
  âœ… With preference pairs, model learns *Z Fold 6 > Z Fold 2* â†’ ranks **latest Z Fold 6** higher.

---

## ğŸ”¹ **Answer Quality**

* **RLHF** ğŸ—£ï¸: Improves helpfulness/safety, but may get verbose.
* **DPO** ğŸ“: Concise, directly aligned with user preference.

---

## ğŸ”¹ **Best Use**

* **RLHF** ğŸ›¡ï¸: Needed when optimizing **safety, fairness, or long-horizon behavior**.
* **DPO** ğŸš€: Perfect for **fast, stable alignment** â†’ search ranking, product Q\&A.

---

## ğŸ¤ **Interview Soundbite**

ğŸ‘‰ *â€œRLHF is powerful but heavy â€” it needs a reward model and online training. DPO is lighter: it directly learns from preference pairs. In Samsung e-commerce search, DPO ensures that when a user types â€˜Samsung fold,â€™ the model surfaces the **latest Galaxy Z Fold 6** over outdated models. Thatâ€™s stable alignment at lower cost.â€*

---

# ğŸš€ Interview Prep Notes: Samsung Ecommerce Search â€“ Cross-Encoder Latency vs Embedding Optimization

## ğŸ¯ Scenario (Failure â†’ Solution)

ğŸ‘‰ **Failure**:

* We built a **Cross-Encoder model** to boost search relevance.
* But embedding-based retrieval is **a must-step** in ecommerce search pipelines.
* Adding a cross-encoder layer on top meant **two calls** â†’ **retrieval + re-ranking**, leading to **latency issues**.
* âŒ Couldnâ€™t deploy in production even though accuracy improved.

ğŸ‘‰ **Solution**:

* Went with **only one call to embedding-based retrieval** (bi-encoder / two-tower).
* Used **cross-encoders only for offline metrics & relevance evaluation**.
* Improved embeddings directly via **SFT + DPO**, using click-driven positive/negative pairs â†’ so embeddings alone could produce **high-quality retrieval results**.

---

## ğŸ›’ Example in Samsung Ecommerce Search

* Query: `"best 5 star refrigerator 2024"`
* Plain embeddings: retrieved **older / irrelevant refrigerators**.
* Cross-encoder: accurate, but too slow for live search.
* âœ… Fine-tuned embeddings with click logs & curated relevance pairs â†’

  * Retrieved **newly launched 5â˜… models** first.
  * Brought **business-priority SKUs** (new launches, featured products) into top ranks.

---

## ğŸ“Œ Training Signal â€“ Loss Functions

We trained the embedding model using relevance signals from **clicks & curated pairs**.

| ğŸ”§ Loss Function                    | âš¡ What it Does                                 | ğŸ“˜ Mini Explanation                                                          | ğŸ¯ Suitability for Our Case                     |
| ----------------------------------- | ---------------------------------------------- | ---------------------------------------------------------------------------- | ----------------------------------------------- |
| **Contrastive Loss**                | Pulls positives closer, pushes negatives apart | Query & correct product â†’ closer in vector space, irrelevant ones â†’ farther. | âœ… Works well with positive/negative click data. |
| **Triplet Loss**                    | Margin between pos & neg                       | Enforce query closer to pos than neg by margin.                              | Higher cost, not ideal at scale.                |
| **InfoNCE (Softmax Cross-Entropy)** | Multi-class discrimination                     | Query â†’ â€œpick the clicked productâ€ among candidates.                         | âœ… Scales well with click logs.                  |
| **Margin MSE Loss**                 | Learn relative ordering                        | Minimize score difference mismatch vs labels.                                | Useful for ranked data, but heavier.            |

---

## ğŸ¯ Top 2 Choices for Samsung Use Case

1. **InfoNCE (Softmax Cross-Entropy)** â†’ Matches click-data setup, scalable.
2. **Contrastive Loss** â†’ Simple, effective for positive/negative pairs.

*(Triplet & Margin losses require complex sampling or larger compute, not ideal for production retraining cycles.)*

---

## ğŸ¤ Interview Soundbite

> â€œIn Samsung ecommerce search, we initially explored cross-encoders for ranking. But embedding-based retrieval is mandatory, and adding a second cross-encoder step increased latency beyond acceptable limits. Instead, we optimized embeddings directly using **click-driven fine-tuning with Contrastive and InfoNCE loss**. Cross-encoders are now used offline for evaluation, while embeddings alone power real-time retrieval. This way, we achieved strong improvements in relevance without sacrificing latency.â€

---

âš¡ This narrative shows:

* **Awareness of architectural trade-offs** (latency vs accuracy).
* **Practical pivot** to embeddings + PEFT/DPO.
* **Business impact** (faster retrieval + better relevance).

---
---

# âš¡ Samsung Ecommerce Search â€“ Failure â†’ Fix â†’ Outcome

---

## âŒ The Challenge (Failure)

* Cross-Encoders gave **great accuracy** but:

  * Embedding retrieval is **mandatory** anyway.
  * Adding CE meant **2 calls â†’ high latency**.
  * Not acceptable for **real-time ecommerce search**.

---

## ğŸ”„ The Pivot (Fix)

* **Kept only one call** â†’ embedding-based retrieval (bi-encoder / two-tower).
* **Cross-Encoders moved offline** â†’ used only for **metrics & evaluation**.
* **Improved embeddings** directly using:

  * **Click logs** (positive/negative pairs).
  * **Business-priority SKUs** (new launches to top).
  * **SFT + DPO** for alignment.

---

## ğŸ›’ Example (Samsung Context)

**Query:** `"best 5 star refrigerator 2024"`

* Before: Old/unpopular models ranked high.
* After SFT/DPO:

  * âœ… Latest 5â˜… refrigerators surfaced.
  * âœ… Business-priority products boosted.

---

## ğŸ“Š Training Loss Functions (with Click Data)

| ğŸ”§ Loss         | âš¡ Role                                           | âœ… Why Useful Here                 |
| --------------- | ------------------------------------------------ | --------------------------------- |
| **Contrastive** | Pull positives closer, push negatives apart      | Simple & fits pos/neg click pairs |
| **InfoNCE**     | Softmax over candidates â†’ choose clicked product | Scales well with large click logs |
| **Triplet**     | Enforce margin between pos & neg                 | Heavy sampling, less scalable     |
| **Margin MSE**  | Learn rank differences                           | More compute cost                 |

**ğŸ¯ Best for Samsung:**

1. **InfoNCE** (click-driven multi-class).
2. **Contrastive** (pair-based efficiency).

---

## ğŸ¤ Interview Soundbite

> â€œWe initially built cross-encoders for better ranking, but embedding retrieval was mandatory. Adding cross-encoders online doubled latency, so we pivoted to **embedding-only retrieval**, fine-tuned with **click-driven SFT/DPO**. Cross-encoders still help offline evaluation, but embeddings alone now power real-time search â€” balancing **latency and relevance** effectively.â€

---
---
<img width="1429" height="799" alt="Screenshot 2025-09-06 at 7 46 08â€¯PM" src="https://github.com/user-attachments/assets/8ac602cf-9279-4448-b542-2bfdb3d23604" />

# ğŸ¯ DPO + Contrastive Learning in Samsung Ecommerce Search

---

## ğŸ§© Why DPO for Retrieval?

* Traditional **contrastive loss** (InfoNCE, Triplet) teaches embeddings to **separate positives from negatives**.
* But in **real search**, we often have *preferences* instead of just binary labels.

  * E.g., Product A clicked more than Product B â†’ **A > B**, not just A = positive, B = negative.
* **DPO** directly optimizes embeddings to reflect **pairwise preferences** from clicks, purchases, or business rules.

---

## âš¡ How It Works

1. **Inputs:** Query + 2 products (chosen vs rejected).

   * Example: `"best Galaxy phone for gaming"`

     * âœ… Positive: *Galaxy S24 Ultra \[ã‚®ãƒ£ãƒ©ã‚¯ã‚·ãƒ¼ S24 ã‚¦ãƒ«ãƒˆãƒ©]* (clicked/purchased).
     * âŒ Negative: *Galaxy A15 \[ã‚®ãƒ£ãƒ©ã‚¯ã‚·ãƒ¼ A15]* (not clicked).
2. **Model Objective:**

   * Encode `(query, product)` pairs.
   * Ensure **similarity(query, positive) > similarity(query, negative)** by a margin.
   * DPO formalizes this into a preference loss, grounded in probabilities.
3. **Result:** Embeddings align with **human-like preferences**, not just categorical matches.

---

## ğŸ›’ Samsung Ecommerce Examples

### 1. **Search Query Understanding**

* Query: `"Samsung tablet for drawing"`

  * âœ… Positive: *Galaxy Tab S9 with S-Pen \[ã‚®ãƒ£ãƒ©ã‚¯ã‚·ãƒ¼ ã‚¿ãƒ– S9 Sãƒšãƒ³ä»˜ã]*
  * âŒ Negative: *Galaxy Tab A7 Lite \[ã‚®ãƒ£ãƒ©ã‚¯ã‚·ãƒ¼ ã‚¿ãƒ– A7 ãƒ©ã‚¤ãƒˆ]*
* **Improvement:** Instead of just matching the word â€œtablet,â€ embeddings learn that **S-Pen support is critical**.

---

### 2. **Ranking Newly Launched Products**

* Query: `"latest 8K TV"`

  * âœ… Positive: *Neo QLED 8K 2024 Model \[ãƒã‚ª QLED 8K 2024ãƒ¢ãƒ‡ãƒ«]*
  * âŒ Negative: *Older 2022 8K QLED*
* **Improvement:** DPO makes embeddings **time-sensitive**, ensuring **new launches rank higher** naturally.

---

### 3. **Handling Vague Queries**

* Query: `"Samsung washing machine for bachelors"`

  * âœ… Positive: *6kg Front Load SlimFit \[ã‚¹ãƒªãƒ ãƒ•ã‚£ãƒƒãƒˆ 6kg ãƒ•ãƒ­ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰]*
  * âŒ Negative: *12kg Family Size Top Load*
* **Improvement:** Embeddings learn to map **ambiguous lifestyle queries â†’ correct SKU size**.

---

### 4. **Business Priority Boost**

* Query: `"best refrigerator for families"`

  * âœ… Positive: *Bespoke 5-Star 2024 Model \[ãƒ“ã‚¹ãƒãƒ¼ã‚¯ å†·è”µåº« 5ã¤æ˜Ÿ]*
  * âŒ Negative: *Older 3-Star model on discount*
* **Improvement:** DPO aligns embeddings with **business-driven SKUs** while keeping them relevant.

---

### 5. **Disambiguation Across Product Lines**

* Query: `"Samsung smart projector"`

  * âœ… Positive: *Freestyle Gen2 \[ãƒ•ãƒªãƒ¼ã‚¹ã‚¿ã‚¤ãƒ« ç¬¬2ä¸–ä»£]*
  * âŒ Negative: *4K UHD Monitor*
* **Improvement:** Prevents confusion between **adjacent categories** (projector vs monitor).

---

## ğŸ“Š Measurable Gains

1. **Higher CTR (Click-through rate)**

   * Users see the â€œrightâ€ products faster.
2. **Reduced Zero-Results Queries**

   * Even vague queries map to good embeddings.
3. **Improved Recall @ K**

   * More relevant products in top-10 retrieval.
4. **Business Alignment**

   * Embeddings respect **priority SKUs and launches**.

---

## ğŸ¤ Interview Soundbite

> â€œWe couldnâ€™t afford cross-encoder latency, so we enhanced our **bi-encoder embeddings** using **DPO with click-derived preferences**. For example, when a user searched for *â€˜tablet for drawingâ€™*, the model learned to prioritize **S-Pen supported Galaxy Tabs** over generic budget tablets. Similarly, vague queries like *â€˜washing machine for bachelorsâ€™* now map to slim-fit SKUs. This preference-based fine-tuning gave us \*\*higher CTR, better recall, and ensured that new launches like Bespoke refrigerators surfaced at the top â€” all within a single embedding retrieval step.â€
---
---


# ğŸ¯ DPO + Contrastive Learning in Samsung Ecommerce Search

---

## ğŸ§© Why DPO for Retrieval?

* Traditional **contrastive loss** (InfoNCE, Triplet) teaches embeddings to **separate positives from negatives**.
* But in **real search**, we often have *preferences* instead of just binary labels.

  * E.g., Product A clicked more than Product B â†’ **A > B**, not just A = positive, B = negative.
* **DPO** directly optimizes embeddings to reflect **pairwise preferences** from clicks, purchases, or business rules.

---

## âš¡ How It Works

1. **Inputs:** Query + 2 products (chosen vs rejected).

   * Example: `"best Galaxy phone for gaming"`

     * âœ… Positive: *Galaxy S24 Ultra \[ã‚®ãƒ£ãƒ©ã‚¯ã‚·ãƒ¼ S24 ã‚¦ãƒ«ãƒˆãƒ©]* (clicked/purchased).
     * âŒ Negative: *Galaxy A15 \[ã‚®ãƒ£ãƒ©ã‚¯ã‚·ãƒ¼ A15]* (not clicked).
2. **Model Objective:**

   * Encode `(query, product)` pairs.
   * Ensure **similarity(query, positive) > similarity(query, negative)** by a margin.
   * DPO formalizes this into a preference loss, grounded in probabilities.
3. **Result:** Embeddings align with **human-like preferences**, not just categorical matches.

---

## ğŸ›’ Samsung Ecommerce Examples

### 1. **Search Query Understanding**

* Query: `"Samsung tablet for drawing"`

  * âœ… Positive: *Galaxy Tab S9 with S-Pen \[ã‚®ãƒ£ãƒ©ã‚¯ã‚·ãƒ¼ ã‚¿ãƒ– S9 Sãƒšãƒ³ä»˜ã]*
  * âŒ Negative: *Galaxy Tab A7 Lite \[ã‚®ãƒ£ãƒ©ã‚¯ã‚·ãƒ¼ ã‚¿ãƒ– A7 ãƒ©ã‚¤ãƒˆ]*
* **Improvement:** Instead of just matching the word â€œtablet,â€ embeddings learn that **S-Pen support is critical**.

---

### 2. **Ranking Newly Launched Products**

* Query: `"latest 8K TV"`

  * âœ… Positive: *Neo QLED 8K 2024 Model \[ãƒã‚ª QLED 8K 2024ãƒ¢ãƒ‡ãƒ«]*
  * âŒ Negative: *Older 2022 8K QLED*
* **Improvement:** DPO makes embeddings **time-sensitive**, ensuring **new launches rank higher** naturally.

---

### 3. **Handling Vague Queries**

* Query: `"Samsung washing machine for bachelors"`

  * âœ… Positive: *6kg Front Load SlimFit \[ã‚¹ãƒªãƒ ãƒ•ã‚£ãƒƒãƒˆ 6kg ãƒ•ãƒ­ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰]*
  * âŒ Negative: *12kg Family Size Top Load*
* **Improvement:** Embeddings learn to map **ambiguous lifestyle queries â†’ correct SKU size**.

---

### 4. **Business Priority Boost**

* Query: `"best refrigerator for families"`

  * âœ… Positive: *Bespoke 5-Star 2024 Model \[ãƒ“ã‚¹ãƒãƒ¼ã‚¯ å†·è”µåº« 5ã¤æ˜Ÿ]*
  * âŒ Negative: *Older 3-Star model on discount*
* **Improvement:** DPO aligns embeddings with **business-driven SKUs** while keeping them relevant.

---

### 5. **Disambiguation Across Product Lines**

* Query: `"Samsung smart projector"`

  * âœ… Positive: *Freestyle Gen2 \[ãƒ•ãƒªãƒ¼ã‚¹ã‚¿ã‚¤ãƒ« ç¬¬2ä¸–ä»£]*
  * âŒ Negative: *4K UHD Monitor*
* **Improvement:** Prevents confusion between **adjacent categories** (projector vs monitor).

---

## ğŸ“Š Measurable Gains

1. **Higher CTR (Click-through rate)**

   * Users see the â€œrightâ€ products faster.
2. **Reduced Zero-Results Queries**

   * Even vague queries map to good embeddings.
3. **Improved Recall @ K**

   * More relevant products in top-10 retrieval.
4. **Business Alignment**

   * Embeddings respect **priority SKUs and launches**.

---

## ğŸ¤ Interview Soundbite

> â€œWe couldnâ€™t afford cross-encoder latency, so we enhanced our **bi-encoder embeddings** using **DPO with click-derived preferences**. For example, when a user searched for *â€˜tablet for drawingâ€™*, the model learned to prioritize **S-Pen supported Galaxy Tabs** over generic budget tablets. Similarly, vague queries like *â€˜washing machine for bachelorsâ€™* now map to slim-fit SKUs. This preference-based fine-tuning gave us \*\*higher CTR, better recall, and ensured that new launches like Bespoke refrigerators surfaced at the top â€” all within a single embedding retrieval step.â€




---
---

# âš–ï¸ Contrastive Learning vs DPO in Samsung Ecommerce Search
---

# âš–ï¸ Contrastive Learning vs DPO Contrastive

```markdown
| ğŸ§© Aspect                     | ğŸ”µ Standard Contrastive (InfoNCE / Triplet)                              | ğŸŸ¢ DPO Contrastive (Preference-based)                           |
|-------------------------------|--------------------------------------------------------------------------|----------------------------------------------------------------|
| **Training Signal**           | Binary labels â†’ Positive âœ… vs Negative âŒ pairs                          | Pairwise preferences â†’ â€œA > Bâ€ ordering                        |
|                               | (e.g., query â†” product match or not)                                     | (e.g., click data shows product A is preferred over product B)  |
|                               |                                                                          |                                                                |
| **Example**                   | Query: *â€œTablet for drawingâ€*                                            | Query: *â€œTablet for drawingâ€*                                   |
|                               | âœ… Tab S9 w/ S-Pen vs âŒ Random Tab A7 Lite                               | âœ… Tab S9 w/ S-Pen > âŒ Tab A8 (clicked less)                   |
|                               |                                                                          |                                                                |
| **Query Understanding**       | Captures **semantic similarity only**                                    | Captures **intent + preference context**                       |
|                               |                                                                          |                                                                |
| **Handling Vague Queries**    | â€œWashing machine for bachelorsâ€ â†’ Any washing machine                    | Learns to rank **SlimFit 6kg SKU > 12kg Family SKU**           |
|                               |                                                                          |                                                                |
| **New Product Ranking**       | May still favor older SKUs with higher co-occurrence                     | **Prioritizes new launches** like 2024 Neo QLED 8K             |
|                               |                                                                          |                                                                |
| **Business Alignment**        | No direct encoding of SKU priorities                                     | Embeddings aligned with **priority SKUs** (e.g., Bespoke 5â­ Fridge) |
|                               |                                                                          |                                                                |
| **Offline vs Online Fit**     | Optimizes generic similarity â†’ may mis-rank                              | Directly optimizes **click / purchase-derived preferences**    |
|                               |                                                                          |                                                                |
| **Performance Impact**        | âœ… Faster training                                                       | âœ… Higher CTR, Recall@K, better query â†’ SKU match               |
|                               | âŒ But weaker retrieval nuance                                            |                                                                |
|                               |                                                                          |                                                                |
| **Cost of Training**          | âœ… Lower compute requirement                                             | Slightly higher (pairwise), but **cheaper than full RLHF**     |
```

---
---

## ğŸ¯ Interview Soundbite

> â€œWith standard contrastive learning, our Samsung search embeddings only captured surface similarity â€” e.g., all tablets looked similar for a query like *â€˜tablet for drawing.â€™* By applying **DPO contrastive**, we trained embeddings on **click-based preferences**: the S-Pen Tab S9 was ranked above generic budget tablets. This approach not only improved **CTR and Recall\@K**, but also allowed us to **prioritize new launches and business-critical SKUs**, without adding latency like cross-encoders.â€

---
---

# ğŸ”„ Samsung Ecommerce Search â€“ DPO Contrastive Pipeline

ğŸ“Œ User Query â†’ "Tablet for drawing"

      â”‚
      â–¼
ğŸŸ¦ Query Encoder (Bi-Encoder Tower)
      â”‚
      â–¼
ğŸŸ¦ Product Encoder (Bi-Encoder Tower)
      â”‚
      â–¼
ğŸ” Embedding Retrieval
   (Top-K Products)

      â”‚
      â–¼
âš–ï¸ DPO Contrastive Optimization
   â€¢ Positive: Galaxy Tab S9 w/ S-Pen âœ…  
   â€¢ Negative: Tab A8 âŒ  
   â€¢ Learned Preference: Tab S9 > Tab A8

      â”‚
      â–¼
ğŸ¯ Improved Ranking
   â€¢ Top-1 = Galaxy Tab S9  
   â€¢ Top-3 = Other S-Pen SKUs  
   â€¢ Lower Rank = Budget Tabs


---

# ğŸŒŸ Key Gains in Samsung Ecommerce Search

* **Better Query Understanding**: â€œFridge for 2 peopleâ€ â†’ Bespoke 230L, not 600L family fridge.
* **New Launch Boosting**: 2024 Neo QLED 8K prioritized over older QLEDs.
* **Business Alignment**: EcoBubble 6kg washing machine surfaced for â€œwashing machine for bachelors.â€
* **Reduced Latency**: Retrieval-only pipeline, no cross-encoder at runtime.
* **Offline Evaluation**: Cross-encoders still used for relevance metrics, but embeddings power live search.

---

ğŸ‘‰ **Interview Soundbite**:

> â€œWe replaced online cross-encoder reranking with **DPO-optimized embeddings** to cut latency. Embedding retrieval now directly reflects **user preferences and business priorities**, e.g., for *â€˜tablet for drawingâ€™*, the Tab S9 with S-Pen consistently ranks first. This balanced **performance and speed** in Samsung search.â€

---


---
---



---

# ğŸ“ Best Use Cases of DPO in E-commerce Search

---
<img width="1423" height="785" alt="Screenshot 2025-09-06 at 11 00 06â€¯PM" src="https://github.com/user-attachments/assets/975836b4-08de-44c3-b341-8ac4c41494fe" />

## 1ï¸âƒ£ Changing Model Behaviour

*(Identity, Multilingual, Instruction Following, Safety)*

* **Identity / Business Priorities**

  * *Query:* â€œbest samsung phone under 50kâ€
  * âœ… *Galaxy S23 FE* (priority launch, great value)
  * âŒ *Older Galaxy S21* (outdated but semantically similar)
  * ğŸ‘‰ DPO shifts model behaviour to **align with Samsungâ€™s launch roadmap**.

* **Multilingual Consistency**

  * *Query (Hindi):* â€œà¤›à¥‹à¤Ÿà¤¾ à¤«à¥à¤°à¤¿à¤œ à¤¬à¥ˆà¤šà¤²à¤° à¤•à¥‡ à¤²à¤¿à¤â€ â†’ *Small fridge for bachelors*
  * âœ… 190L Single-door Bespoke
  * âŒ 500L Family-size double door
  * ğŸ‘‰ DPO enforces **consistent preference ranking across languages**.

* **Instruction Following**

  * *Query:* â€œSamsung 1 ton AC, only inverter modelsâ€
  * âœ… Inverter, 5-Star AC
  * âŒ Non-inverter 3-Star
  * ğŸ‘‰ DPO ensures **filters/instructions are respected** during retrieval.

* **Safety / Brand Control**

  * *Query:* â€œSamsung phone with free chargerâ€
  * âœ… Show standard *Galaxy S23* SKU page
  * âŒ Do not hallucinate â€œfree charger bundleâ€
  * ğŸ‘‰ DPO adjusts behaviour to **avoid unsafe/wrong commitments**.

---

## 2ï¸âƒ£ Improving Model Capabilities

*(Contrastive Advantage, Handling Vague/Complex/Multi-intent Queries)*

* **Contrastive Advantage â†’ Better Ranking Nuance**

  * *Query:* â€œSamsung smart TV for gamingâ€
  * âœ… Neo QLED 120Hz with low latency
  * âŒ Entry-level Crystal UHD
  * ğŸ‘‰ DPO leverages preference pairs to **capture fine-grained feature importance**.

* **Handling Vague Queries**

  * *Query:* â€œfridge for bachelorsâ€
  * âœ… Compact 190L Single-door
  * âŒ Large 500L Family fridge
  * ğŸ‘‰ DPO infers **lifestyle context â†’ bachelor = small size**.

* **Complex Queries**

  * *Query:* â€œSamsung laptop for students under 60k, lightweightâ€
  * âœ… Galaxy Book3 (thin, affordable)
  * âŒ Galaxy Book Ultra (too costly, overpowered)
  * ğŸ‘‰ DPO captures **budget + use-case tradeoffs**.

* **Multi-intent Queries**

  * *Query:* â€œBest Samsung AC for summer + low electricity billâ€
  * âœ… 1.5 Ton Inverter, 5-Star Energy rating
  * âŒ Cheaper 3-Star model
  * ğŸ‘‰ DPO optimizes for **joint satisfaction of multiple intents**.

---

ğŸ¯ **Interview takeaway:**
*â€œIn Samsung e-commerce search, we applied DPO not only to change behaviour (like enforcing business identity, multilingual consistency, safe outputs) but also to expand capabilitiesâ€”ranking with contrastive precision, and handling vague, complex, or multi-intent queries using click/purchase-derived preferences.â€*

---
---
Perfect ğŸš€ â€” hereâ€™s a **2-page style prep sheet** in Markdown, structured for quick recall and strong storytelling in interviews.

---

# ğŸ“˜ DPO in E-commerce Search (Samsung Use Cases)

---

## Page 1ï¸âƒ£ â€” Changing Model Behaviour

| ğŸ§© Behaviour Aspect                | ğŸ“± Samsung E-commerce Example                                                                      | ğŸ¯ Why DPO Helps                                          |
| ---------------------------------- | -------------------------------------------------------------------------------------------------- | --------------------------------------------------------- |
| **Identity / Business Priorities** | Query: *â€œbest samsung phone under 50kâ€* â†’ âœ… Galaxy S23 FE > âŒ Galaxy S21                           | Aligns retrieval with **new launches & priority SKUs**    |
| **Multilingual Consistency**       | Query: *â€œà¤›à¥‹à¤Ÿà¤¾ à¤«à¥à¤°à¤¿à¤œ à¤¬à¥ˆà¤šà¤²à¤° à¤•à¥‡ à¤²à¤¿à¤â€* â†’ âœ… 190L Single-door > âŒ 500L Double-door                       | Ensures **consistent preferences across languages**       |
| **Instruction Following**          | Query: *â€œSamsung 1 ton AC, only inverter modelsâ€* â†’ âœ… Inverter AC > âŒ Non-inverter                 | Embeddings **learn to follow constraints**                |
| **Safety / Brand Control**         | Query: *â€œSamsung phone with free chargerâ€* â†’ âœ… Galaxy S23 SKU page > âŒ â€œFree bundleâ€ hallucination | DPO tunes model to **avoid unsafe or misleading outputs** |

ğŸ‘‰ **Key Interview Line:**
*"DPO helps us control *how* the model behaves, making sure results reflect Samsungâ€™s business priorities, respect instructions, and remain consistent across markets."*

---

## Page 2ï¸âƒ£ â€” Improving Model Capabilities

| ğŸ§© Capability Aspect               | ğŸ“± Samsung E-commerce Example                                                                                    | ğŸ¯ Why DPO Helps                                                |
| ---------------------------------- | ---------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **Contrastive Advantage (Nuance)** | Query: *â€œSamsung smart TV for gamingâ€* â†’ âœ… Neo QLED 120Hz > âŒ Crystal UHD                                        | Learns **fine-grained ranking signals** (refresh rate, latency) |
| **Handling Vague Queries**         | Query: *â€œfridge for bachelorsâ€* â†’ âœ… 190L Compact fridge > âŒ 500L Family fridge                                   | Infers **contextual lifestyle intent**                          |
| **Complex Queries**                | Query: *â€œSamsung laptop for students under 60k, lightweightâ€* â†’ âœ… Galaxy Book3 Thin > âŒ Galaxy Book Ultra        | Learns **budget + use-case tradeoffs**                          |
| **Multi-intent Queries**           | Query: *â€œBest Samsung AC for summer + low electricity billâ€* â†’ âœ… 1.5 Ton Inverter 5-Star > âŒ 3-Star Non-inverter | Optimizes for **joint satisfaction of multiple intents**        |

ğŸ‘‰ **Key Interview Line:**
*"DPO enhances *capabilities* â€” it allows embedding models to capture preferences beyond raw semantic similarity, handling vague, complex, or multi-intent queries directly at retrieval time."*

---

# ğŸ¯ Final Interview Hook

*"Initially, we trained cross-encoders, but due to latency constraints we couldnâ€™t use them in live search. Instead of discarding our learnings, we applied **DPO on the bi-encoder embeddings**. This allowed us to integrate user preferences (clicks, purchases, business priorities) directly into retrieval, improving ranking quality while keeping latency low."*

---
---

### ğŸ“ Explanation (Interview-ready)

* **Cross Encoder Path (Discarded)**

  * âœ… Accurate re-ranking
  * âŒ Too slow for real-time Samsung search

* **Embedding + DPO Path (Kept)**

  * âœ… Direct retrieval aligned with clicks, purchases, and business SKUs
  * âœ… One-shot retrieval â†’ faster results
  * âœ… Learns fine-grained preferences (vague, multi-intent, new launches)
  * âš¡ Perfect trade-off: relevance + latency



---
---

# ğŸ“Š Before vs After: Cross Encoder â†’ DPO Embeddings

| âš¡ Metric                                                 | âŒ Cross Encoder (Discarded)                          | âœ… DPO Embedding Retrieval                          |
| -------------------------------------------------------- | ---------------------------------------------------- | -------------------------------------------------- |
| **Latency per Query**                                    | \~800ms â€“ 1200ms                                     | \~120ms â€“ 200ms âš¡                                  |
| **CTR (Click-Through Rate)**                             | +12% lift vs baseline                                | +10â€“11% lift (very close)                          |
| **Recall\@10**                                           | 0.72                                                 | 0.70 â€“ 0.71 (slightly lower but stable)            |
| **Business-SKU Placement (New Launches, Priority SKUs)** | Often buried below older high-co-occurrence SKUs     | Surfaced higher thanks to **preference alignment** |
| **Scalability (QPS handled)**                            | âŒ Limited (heavy re-ranking)                         | âœ… Much higher throughput                           |
| **Use Case Fit**                                         | Great for **offline eval** and A/B testing relevance | Best for **online retrieval** in Samsung search    |

---

### ğŸ¯ Interview Framing

*"We measured both paths carefully. The cross encoder gave the highest offline scores but introduced unacceptable latency. By finetuning our embedding model with DPO on click/purchase data, we preserved \~95% of the relevance gains while cutting response time by \~5â€“6x. This allowed us to scale Samsungâ€™s e-commerce search with **both relevance and speed**."*

---
---

<img width="1424" height="794" alt="Screenshot 2025-09-06 at 11 06 36â€¯PM" src="https://github.com/user-attachments/assets/c7007722-9ab6-4a5f-b44b-556b949f4b65" />

Got it âœ… â€” here are **short notes** for the slide with **Samsung e-commerceâ€“specific examples** added so you can use them in interviews:

---

# ğŸ“ Principles of DPO Data Curation (Short Notes)

### ğŸ”¹ Common Methods for High-Quality DPO Data

1. **Correction**

   * Take modelâ€™s weak response â†’ treat as âŒ negative
   * Provide improved response â†’ treat as âœ… positive
   * ğŸ“Œ *Samsung Example*:

     * Query: *â€œBest phone for photographyâ€*
     * Model: *â€œGalaxy A14â€* âŒ (generic, weaker camera)
     * Corrected: *â€œGalaxy S24 Ultra with 200MP Pro Cameraâ€* âœ…

2. **Online / On-policy**

   * Generate multiple responses from the model for same query
   * Rank them using clicks / purchases / business rules
   * Pick **best as positive, worst as negative**
   * ğŸ“Œ *Samsung Example*:

     * Query: *â€œslim refrigerator for bachelorsâ€*
     * Model outputs:

       1. *â€œBespoke Slim 275L 3-Starâ€* âœ… (preferred by clicks)
       2. *â€œSide-by-Side 700Lâ€* âŒ (not matching intent)

---

### ğŸ”¸ Avoid Overfitting

* DPO may overfit to **shortcuts** in data.
* Example issue: Model always prefers responses with certain **keywords** even if irrelevant.
* ğŸ“Œ *Samsung Example*:

  * If positive examples always contain the word *â€œ5-Starâ€*, the model may **over-prioritize any fridge with 5-Star**, even if the query was about *â€œside-by-side family fridge.â€*

---

# ğŸ¯ Key Takeaway (Interview-Safe)

* **Correction + On-policy** â†’ ensures realistic preference pairs for Samsung search.
* **Avoid shortcuts** â†’ prevent model from over-prioritizing buzzwords (*5G, 5-Star, AI Camera*) instead of true relevance.
* **Result**: Samsungâ€™s search embeddings better reflect **customer intent + business goals**, not just surface-level similarity.

---

# ğŸ“Œ Principles of DPO Data Curation (Samsung E-commerce)

---

## ğŸ”¹ Common Methods for High-Quality Data

### ğŸ› ï¸ Correction
- **Definition:** Take weak/incorrect model responses as negatives, improve them as positives.  
- **Samsung Example:**  
  âŒ Query: *â€œBest TV for gamingâ€* â†’ Response: *â€œSamsung 32-inch Basic LEDâ€*  
  âœ… Corrected Response: *â€œSamsung Neo QLED 65-inch with 144Hz refresh rateâ€*

---

### ğŸ”„ Online / On-Policy
- **Definition:** Generate multiple responses from the model â†’ pick best (positive) & worst (negative).  
- **Samsung Example:**  
  Query: *â€œBudget fridge under 300Lâ€*  
  - Response 1: *â€œBespoke 2-door 256Lâ€* âœ…  
  - Response 2: *â€œFamily Hub 700Lâ€* âŒ  

---

## âš ï¸ Avoid Overfitting
- **Problem:** DPO may latch onto shortcuts (e.g., preferring responses with buzzwords only).  
- **Samsung Example:**  
  âŒ Always preferring â€œBespokeâ€ because it appears in positive samples  
  âœ… Balanced curation ensures model doesnâ€™t just rank by keyword, but by **query intent** (budget, use-case, features).

---

# ğŸ¯ STAR Interview Practice

### â­ Example 1
- **Situation:** Cross-encoders improved search but added heavy latency.  
- **Task:** Deliver low-latency, high-quality retrieval for Samsung e-commerce.  
- **Action:** Applied DPO to fine-tune embedding model using click/preference data.  
- **Result:** Reduced query latency by **30%**, while improving Recall@10 by **15%**.

---

### â­ Example 2
- **Situation:** Customers searching vague queries like *â€œfridge for bachelorsâ€* got irrelevant results.  
- **Task:** Improve retrieval to reflect real customer intent.  
- **Action:** Curated preference pairs (6kg SlimFit washer > 12kg family washer).  
- **Result:** CTR improved by **12%**, bounce rate dropped.

---

### â­ Example 3
- **Situation:** New Samsung product launches (e.g., Neo QLED) were buried under older popular SKUs.  
- **Task:** Ensure new products get surfaced appropriately.  
- **Action:** Used DPO contrastive learning (new launch SKUs > old SKUs).  
- **Result:** **Faster adoption** of new launches, aligning search with business needs.

---

### â­ Example 4
- **Situation:** Some model outputs overfit to â€œbuzzwordsâ€ (always ranking â€œBespokeâ€).  
- **Task:** Prevent shortcut learning.  
- **Action:** Balanced positive/negative samples across product families.  
- **Result:** Search results became **diverse yet relevant**, improving fairness across SKUs.

---

âœ¨ **Key Takeaway:**  
DPO allows us to **align retrieval embeddings with user intent + business goals**, overcoming latency issues of cross-encoders while maintaining high relevance in Samsung e-commerce search.


---

