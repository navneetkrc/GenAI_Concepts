---

# ğŸ” Reinforcement Learning for LLMs: Online vs Offline

---

<img width="1433" height="779" alt="Screenshot 2025-09-07 at 2 05 14â€¯PM" src="https://github.com/user-attachments/assets/31bdb1bc-74e3-435d-82e6-bcc7a999f9fe" />




## ğŸŸ  **Online Learning**

âœ¨ *Learns in real-time with continuous feedback*

* âš¡ **How it works:** Model generates new responses â†’ collects rewards â†’ updates weights live.
* ğŸ“± **Samsung Example:** When users start searching for *â€œGalaxy Z Fold6â€*, search instantly adapts to rank foldables higher.
* âœ… **Pros:**

  * ğŸ”¥ Adapts quickly to trending products (e.g., festival offers, seasonal sales).
  * ğŸ†• Keeps recommendations fresh.
* âš ï¸ **Cons:**

  * âŒ Risk of reinforcing noisy/incorrect signals.
  * ğŸ’» High computational cost in real-time.

---

## ğŸ”µ **Offline Learning**

ğŸ“š *Learns from pre-collected historical data*

* âš¡ **How it works:** Uses fixed promptâ€“responseâ€“reward tuples. No fresh generation during training.
* ğŸ“± **Samsung Example:** Training on past queries like *â€œGalaxy S23 Ultraâ€ â†’ clicks on accessories* before deployment.
* âœ… **Pros:**

  * ğŸ›¡ï¸ Stable & safe (no live instability).
  * ğŸ“Š Leverages massive logs (search, clicks, purchases).
* âš ï¸ **Cons:**

  * ğŸ¢ Slow to adapt to **new launches** (e.g., Fold6 just released).
  * â³ May miss sudden shifts in user behavior.

---

## ğŸ“Œ **Interview Takeaway**

ğŸ‘‰ In Samsung e-commerce, we combine both:

* **Offline RL** for stable learning from billions of historical logs.
* **Online RL** for fast adaptation to **new launches** and **seasonal demand shifts**.

---
---
# ğŸ¤– Online RL: Let Model Explore Better Responses

---

<img width="1056" height="595" alt="Screenshot 2025-09-07 at 2 11 05â€¯PM" src="https://github.com/user-attachments/assets/ba190c81-c62f-40d7-b291-1db468a152b6" />

---


## ğŸ”„ **Core Flow**

1. ğŸ“¥ **Batch of Prompts**
   â†’ Users searching on Samsung.com (*â€œBest phone for gamingâ€*, *â€œenergy-efficient fridgeâ€*).

2. âš™ï¸ **Language Model Generates Responses**
   â†’ Suggests ranked product lists (e.g., Galaxy S23 Ultra, Galaxy A54).

3. ğŸ¯ **Reward Function Evaluates**
   â†’ Based on **clicks, add-to-cart, purchase rate**.

   * Example: If more users click *Galaxy S23 Ultra* over *Galaxy A54*, that gets **higher reward**.

4. ğŸ” **Update Weights**
   â†’ Model learns **real-time preference signals**.

   * Next time, it shows **S23 Ultra higher for gaming queries** automatically.

---

## ğŸ“± **Samsung e-Commerce Example**

* Query: *â€œTablet for drawingâ€* ğŸ¨

  * **Generated Response:** Galaxy Tab A8 + Galaxy Tab S9 Ultra.
  * **Reward Function:** Users overwhelmingly click Tab S9 Ultra (with S-Pen).
  * **Update:** Model learns â†’ Rank **S9 Ultra > A8** for drawing-related queries.

* Query: *â€œFridge for bachelorsâ€* ğŸ§‘â€ğŸ³

  * **Generated Response:** Large 12kg Bespoke Fridge + SlimFit 6kg Fridge.
  * **Reward Function:** SlimFit gets more clicks/purchases.
  * **Update:** Model prioritizes **SlimFit SKU** in future for â€œbachelorâ€ queries.

---

## ğŸ“Œ **Key Insight**

ğŸ‘‰ Online RL allows Samsungâ€™s search engine to **self-correct and adapt in real-time**:

* âš¡ New product launches (e.g., Fold6, Bespoke AI Oven) get boosted quickly.
* ğŸ“Š Aligns with **true customer intent** from clicks & purchases.

---
---

# ğŸ“Š Samsung e-Commerce: Online RL in Action

| âš¡ Scenario                              | âŒ Before RL (Static Search)                | âœ… After RL (Online RL Updates)                                         |
| --------------------------------------- | ------------------------------------------ | ---------------------------------------------------------------------- |
| ğŸ® **Query: "Best phone for gaming"**   | S23 Ultra ranked below A54 (lexical bias). | S23 Ultra moves up after clicks â†’ optimized for gaming intent.         |
| ğŸ¨ **Query: "Tablet for drawing"**      | Tab A8 shown equally with Tab S9 Ultra.    | Tab S9 Ultra ranked higher (rewarded by user clicks on S-Pen feature). |
| ğŸ§‘â€ğŸ³ **Query: "Fridge for bachelors"** | Large Bespoke fridge ranked top.           | SlimFit fridge boosted â†’ aligns with small household preference.       |
| ğŸ›’ **Query: "Budget washing machine"**  | Expensive AI Combo sometimes shown.        | Model adapts: ranks 7kg EcoBubble as top pick (higher conversion).     |
| ğŸ”¥ **New Launch: Galaxy Z Fold6**       | Low rank initially due to no history.      | Clicks & early adoption boost Fold6 visibility within days.            |

---

## ğŸš€ **Key Takeaway for Interviews**

* **Situation:** Samsung search engine struggled with static retrieval.
* **Task:** Improve ranking quality and adapt to real customer behavior.
* **Action:** Applied Online RL â†’ model generated responses, reward functions captured clicks/purchases, weights updated iteratively.
* **Result:** Higher **CTR, conversions, and relevance** across phones, tablets, and appliances.

---
---

<img width="1424" height="792" alt="Screenshot 2025-09-07 at 2 14 18â€¯PM" src="https://github.com/user-attachments/assets/061447d4-f119-4f52-b9a4-2b4f99faaccb" />


---

# ğŸ¯ Reward Function in Online RL

**Option 1: Trained Reward Model**

---

## ğŸ“ How It Works

1. ğŸ“„ Two responses (e.g., product rankings or summaries) are **judged by humans/customers**.
2. ğŸ§  Reward model assigns scores â†’ `r_j` vs `r_k`.
3. âš–ï¸ Loss computed: `log(Ïƒ(r_j - r_k))` â†’ updates model.
4. ğŸ”„ Model learns which response is **preferred**.

---

## âš¡ Pros & Cons

* âœ… Initialized from **instruct model**, fine-tuned on preference data.
* ğŸŒ Works well for **open-ended tasks** (e.g., ranking, personalization).
* ğŸ”’ Improves **chat quality & safety**.
* âš ï¸ Less accurate for **strict correctness domains** (e.g., coding, math).

---

## ğŸ“¦ Samsung e-Commerce Examples

| ğŸ›’ Query                                | âŒ Less Preferred Response                        | âœ… More Preferred Response                                                 |
| --------------------------------------- | ------------------------------------------------ | ------------------------------------------------------------------------- |
| **"Phone for photography"** ğŸ“¸          | Ranks Galaxy A14 higher due to keyword â€œcameraâ€. | Prioritizes Galaxy S24 Ultra (rewarded by users for pro camera features). |
| **"Budget fridge for bachelors"** ğŸ§‘â€ğŸ³ | Suggests 500L Family Fridge (irrelevant).        | Rewards 190L Single-Door model (clicks show bachelor preference).         |
| **"Tablet for drawing"** ğŸ¨             | Suggests Tab A8 (no stylus).                     | Rewards Tab S9 Ultra with S-Pen support.                                  |
| **"Latest Samsung TV"** ğŸ“º              | Shows older 2022 QLED models.                    | Rewards 2024 Neo QLED/Frame TV (human preference = newest).               |

---

## ğŸš€ Interview Takeaway

* **Situation:** Search often misranks Samsung SKUs due to semantic overlap.
* **Task:** Learn **true user intent** from clicks/purchases.
* **Action:** Applied a **trained reward model** â†’ compares positive vs negative responses.
* **Result:** Higher **CTR, conversions, and alignment** with user preferences.

---
---

<img width="1432" height="781" alt="Screenshot 2025-09-07 at 2 16 47â€¯PM" src="https://github.com/user-attachments/assets/e13c3876-418c-4c67-b318-1303ba805c95" />

---

# ğŸ·ï¸ **Why Verifiable Rewards (Ground Truth) Matter â€” Short & Sharp**

**Core idea:**
When you can *directly check* a modelâ€™s output against a known-correct answer (a ground truth or unit test), the reward signal is deterministic, low-noise, auditable, and safe â€” often *more useful* than an opaque, large learned reward model.

---

## ğŸ” Verifiable vs Learned Reward Models â€” Quick Comparison

| âœ… Aspect                        |              ğŸ”µ **Verifiable (Ground-truth / Unit-tests)** |              ğŸŸ£ **Learned Reward Models (Large / Complex)** |
| ------------------------------- | ---------------------------------------------------------: | ----------------------------------------------------------: |
| **Signal quality**              |                                Deterministic / low-noise âœ… |                        Noisy, predictive â€” can be biased âš ï¸ |
| **Interpretability**            |                    Transparent: pass/fail or exact score âœ… |                                  Opaque and hard to audit â“ |
| **Robustness to hacking**       |                 Harder to game if tests cover edge-cases âœ… |                    Easily gamed / adversarially exploited âŒ |
| **Scalability cost**            |                     One-time test creation; cheap to run âœ… |   Requires many labels, continual retraining â€” expensive ğŸ’¸ |
| **Handling distribution shift** |                   Traceable failures â€” easy to add tests âœ… |          Model drifts can break reward alignment silently â— |
| **Best fit**                    | Math, coding, factual QA, retrieval, unit-testable tasks âœ… | Complex human-preference signals, nuanced quality judgments |

---

## ğŸ§© Why prefer verifiable rewards (concrete reasons)

* **Deterministic feedback:** Exact matches (numbers, labels, test-pass) make training signals consistent.
* **Better debugging & audit trails:** Failed tests show *what* broke and *why*.
* **Safety & alignment:** Easier to enforce constraints (e.g., â€œnever return PIIâ€) with tests.
* **Cost-effective at scale:** Unit tests / label sets run cheaply across many examples.
* **Hard guarantees:** For mission-critical logic (pricing, billing, code), verifiability is essential.

**Interview punchline:**
â€œWhere correctness is binary or verifiable (math, unit-tests, facts), ground-truth rewards give reliable, auditable supervision that learned reward models usually canâ€™t match.â€

---

## ğŸ“š Verifiable Reward Datasets & Test-Style Suites (by domain)

### ğŸ§® **Math & Reasoning**

* **GSM8K** â€” grade-school math word problems with numeric answers (verifiable).
* **MATH** â€” competition-level math problems with definitive solutions.
* **Synthetic arithmetic sets** â€” perfect for deterministic checks.

### ğŸ’» **Code & Programming**

* **HumanEval** (OpenAI) â€” coding problems with unit tests (pass/fail).
* **MBPP** (Mostly Basic Python Problems) â€” unit-test-based verification.
* **APPS** â€” programming problems with test suites for automatic scoring.
  **Why:** Unit tests provide direct reward signals (pass = good, fail = bad).

### ğŸ“– **Reading Comprehension & QA**

* **SQuAD / NaturalQuestions** â€” ground-truth answer spans or exact answers.
* **DROP** â€” discrete reasoning / numeric answers that can be validated.
* **MS MARCO / TREC / BEIR** â€” retrieval datasets with relevance labels (verifiable metrics: MRR, NDCG).

### ğŸ” **Fact-checking / Factuality**

* **FEVER** â€” claim verification with labeled evidence (entailed/refuted).
* **Fact-check datasets** (structured claims + verifiable labels).
  **Why:** You can programmatically check claim vs source evidence.

### ğŸ§¾ **Structured Business Logic**

* **Pricing & billing test suites** â€” synthetic or historical cases with known correct outputs.
* **Inventory & reconciliation tests** â€” deterministic checks (stock math, totals).

---

## ğŸ› ï¸ Best Practices â€” Use Verifiable Rewards Effectively

1. **Start with unit-tests / golden answers** for core correctness paths.
2. **Combine:** use verifiable rewards for correctness + learned reward models for soft preferences (fluency, style).
3. **Cover edge cases:** adversarial/rare-case tests reduce brittleness.
4. **Automate continuous testing:** run tests in CI for model updates.
5. **Version & audit tests:** ground-truth datasets are your safety contract â€” track changes.

---

## âœ… Final Recommendation (one-liner)

**Use verifiable rewards as the foundation for correctness and safety; augment with learned reward models only where human preferences or nuance canâ€™t be strictly specified.**

---

---
---
<img width="1429" height="782" alt="Screenshot 2025-09-07 at 2 35 43â€¯PM" src="https://github.com/user-attachments/assets/dd2968ec-1cc9-43e1-8fbd-f2ceb1a38115" />

---

# ğŸ“˜ **Policy Training in Online RL**

In online reinforcement learning (RL), models are trained by adjusting policies based on rewards and feedback. This slide introduces two key methods: **PPO** and **GRPO**.

---

## âœ… **PPO â€“ Proximal Policy Optimization**

ğŸ¬ **Example â€“ Netflix Movie Recommendations**

* **Policy Model:** Suggests movies based on user query like *â€œBest sci-fi movies to watch this weekendâ€*
* **Reference Model:** A pre-trained recommendation system to maintain consistency with past viewing trends
* **Reward Model:** Checks if the user actually clicks â€œPlayâ€ or watches the movie
* **Value Model:** Estimates long-term user engagement based on watch history

ğŸ”„ **How it works:**

* Compares new recommendations with previous ones using KL divergence
* Uses Generalized Advantage Estimation (GAE) to assess improvements
* Carefully updates suggestions without making radical changes that could confuse users

---

## âœ… **GRPO â€“ Group Relative Policy Optimization**

ğŸ§ **Example â€“ Spotify Playlist Creation**

* Generates playlists for different user groups:

  * ğŸ¸ Rock fans
  * ğŸ¹ Jazz listeners
  * ğŸ¤ Pop enthusiasts
* **Observations (oâ‚, oâ‚‚...oá´³):** Inputs tailored to each groupâ€™s preferences
* **Group Computation:** Aggregates feedback across all users to enhance playlist suggestions

ğŸ’¡ **Why GRPO is useful:**

* Optimizes recommendations for various audience segments at once
* Enables sharing of learning across groups for faster improvements

---

## ğŸ“Š **Key Differences**

| Feature         | âœ… PPO                 | âœ… GRPO                          |
| --------------- | --------------------- | ------------------------------- |
| Scope           | Single user/query     | Multiple user groups            |
| Feedback type   | Personalized          | Collective                      |
| Stability focus | Limits abrupt changes | Encourages group-based learning |
| Applications    | Movie recommendations | Music playlists, news feeds     |

---

## ğŸ”¢ **Mathematical Objective (PPO)**

$$
J_{PPO}(\theta) = \mathbb{E}_{q,o}\left[ \frac{1}{|o|}\sum_{t=1}^{|o|} \min \left( \frac{\pi_\theta(o_t|q, o_{<t})}{\pi_{\text{old}}(o_t|q, o_{<t})} A_t, \text{clip}\left( \frac{\pi_\theta(o_t|q, o_{<t})}{\pi_{\text{old}}(o_t|q, o_{<t})}, 1-\epsilon, 1+\epsilon \right)A_t \right) \right]
$$

ğŸ“Œ Ensures controlled updates to the recommendation system during training.

---

## âœ… **Why This Matters for Platforms Like Netflix, Spotify, and Amazon**

* ğŸ¯ Personalization â†’ Increases user engagement
* ğŸ”„ Smooth updates â†’ Avoids confusing or irrelevant suggestions
* ğŸ¤ Shared learning â†’ Scales improvements across users
* ğŸš€ Efficient training â†’ Better user experience without needing massive individual data

---
---

# ğŸ“˜ **Policy Training in Online RL**

Online Reinforcement Learning (RL) trains models by updating policies based on rewards and feedback. This slide explains two methods: **PPO** and **GRPO**.

---

## âœ… **PPO â€“ Proximal Policy Optimization**

ğŸ“¦ **Use Case â€“ Samsung Product Recommendations**

* **Policy Model:** Generates personalized product suggestions based on user query â€œBest Samsung phone under â‚¹50,000?â€
* **Reference Model:** A pre-trained recommendation system that ensures suggestions align with past patterns.
* **Reward Model:** Checks if the recommended products are clicked or purchased.
* **Value Model:** Estimates long-term satisfaction from these recommendations.

ğŸ”„ **Training Process:**

* Compares current suggestions with older ones using KL divergence.
* Uses Generalized Advantage Estimation (GAE) to assess improvements.
* Updates recommendations carefully without making large disruptive changes.

---

## âœ… **GRPO â€“ Group Relative Policy Optimization**

ğŸ“¦ **Use Case â€“ Samsung Bundle Offers**

* Generates multiple suggestions at once for different customer segments:

  * ğŸ“± Mobile shoppers
  * ğŸ“º TV buyers
  * ğŸ§Š Appliances users
* **Observations (oâ‚, oâ‚‚...oá´³):** Different inputs based on segment preferences.
* **Group Computation:** Aggregates feedback across segments to fine-tune recommendations.

ğŸ’¡ **Why GRPO?**

* Helps Samsung optimize offers for diverse user groups without needing separate training pipelines.
* Encourages sharing of learning across related tasks.

---

## ğŸ“Š **Key Differences**

| Feature         | âœ… PPO                        | âœ… GRPO                            |
| --------------- | ---------------------------- | --------------------------------- |
| Scope           | Single user/query            | Multiple segments                 |
| Feedback type   | Individual                   | Group-based                       |
| Stability focus | Limits large updates         | Encourages cross-segment learning |
| Applications    | Personalized recommendations | Multi-segment offers and bundles  |

---

## ğŸ”¢ **Mathematical Objective (PPO)**

$$
J_{PPO}(\theta) = \mathbb{E}_{q,o}\left[ \frac{1}{|o|}\sum_{t=1}^{|o|} \min \left( \frac{\pi_\theta(o_t|q, o_{<t})}{\pi_{\text{old}}(o_t|q, o_{<t})} A_t, \text{clip}\left( \frac{\pi_\theta(o_t|q, o_{<t})}{\pi_{\text{old}}(o_t|q, o_{<t})}, 1-\epsilon, 1+\epsilon \right)A_t \right) \right]
$$

ğŸ“Œ Controls how much the model is allowed to change during training.

---

## âœ… **Why This Matters for Samsung**

* ğŸ“¦ Better product suggestions â†’ More user satisfaction
* ğŸ“Š Balanced updates â†’ Avoid sudden, confusing changes
* ğŸ¤ Shared learning â†’ Efficient training across segments
* ğŸš€ Scalable personalization â†’ Improved shopping experience across devices

---
<img width="1430" height="789" alt="Screenshot 2025-09-07 at 2 57 31â€¯PM" src="https://github.com/user-attachments/assets/f43c30a5-6966-47fa-b216-ebd5a5148883" />

---

### âœ… **PPO â€“ Proximal Policy Optimization**

**Definition:**
PPO is a reinforcement learning algorithm that stabilizes training by limiting how much the policy is allowed to change between updates, using clipped objectives and divergence penalties.

**Interview Punchline:**
â€œItâ€™s like teaching a model to learn without overreacting â€” ensuring safer, smoother, and more reliable improvements during training.â€

---

### âœ… **GRPO â€“ Group Relative Policy Optimization**

**Definition:**
GRPO extends PPO by aggregating observations and rewards from multiple tasks or user segments, enabling models to learn from shared feedback and improve across groups.

**Interview Punchline:**
â€œItâ€™s the smart way to scale learning â€” letting models collaborate and generalize faster while optimizing for diverse tasks at once.â€

---
---

# ğŸ“Š **Comparison: PPO vs GRPO in Online RL**

| âœ… Feature                | ğŸ“¦ **PPO â€“ Proximal Policy Optimization**                                                                                                    | ğŸ“‚ **GRPO â€“ Group Relative Policy Optimization**                                                                                                          |
| ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ğŸ“– **Main Issue Before** | Training large models led to unstable updates â†’ recommendations or responses would suddenly change, confusing users or degrading performance | Handling diverse user groups or tasks individually was inefficient â†’ models couldnâ€™t learn from related tasks, leading to slow or suboptimal improvements |
| ğŸ§  **How It Solved It**  | Carefully limits policy changes using KL divergence and clipped objectives â†’ ensures updates are smooth and safe                             | Shares information across tasks by grouping observations â†’ accelerates learning and improves performance across segments                                  |
| ğŸ“¦ **Popular Use Cases** | Personalized suggestions, chatbots, reasoning tasks for individuals                                                                          | Multi-task learning, group-based recommendations, shared experiences across users                                                                         |
| ğŸ¤– **Popular LLMs**      | OpenAIâ€™s ChatGPT, Anthropicâ€™s Claude, DeepMindâ€™s Sparrow                                                                                     | Google DeepMindâ€™s Gopher-Cluster variants, Metaâ€™s LLaMA-Group, research prototypes for multi-domain assistants                                            |
| âš™ **Training Method**    | Single-query focus â†’ optimizes step-by-step learning for one interaction at a time                                                           | Group-query focus â†’ computes group rewards and gradients, enabling collaborative learning                                                                 |
| ğŸ“ˆ **Strengths**         | More stable updates, better safety, great for reasoning tasks                                                                                | Faster convergence for multiple tasks, cross-learning, efficient use of feedback                                                                          |

---

### ğŸ“Œ Summary

â¡ **PPO** was designed to address instability during training when models made abrupt updates that could disrupt user experience.
â¡ **GRPO** came later to solve scalability issues when handling multiple tasks or user segments, enabling shared learning across groups while keeping updates smooth.

---
---



