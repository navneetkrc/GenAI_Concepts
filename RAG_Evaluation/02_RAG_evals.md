# ğŸ“‚ **Part 2: RAG Pipeline & Evaluation Metrics**

---

## ğŸ”„ **Basic RAG Pipeline Overview**

<img width="961" height="938" alt="Screenshot 2025-09-13 at 3 05 24â€¯PM" src="https://github.com/user-attachments/assets/49a52292-fae7-4c15-8d64-bda2c7f869b8" />


A typical Retrieval-Augmented Generation (RAG) system follows three key stages that together ensure your LLM is both informed and accurate:

### 1ï¸âƒ£ **Ingestion**

* âœ… Documents are split into meaningful chunks
* âœ… Each chunk is converted into embeddings
* âœ… Organized into an index for efficient search
  
  ğŸ“‚ **Goal:** Prepare data so it can be easily retrieved when needed

---

### 2ï¸âƒ£ **Retrieval**

* âœ… The user query triggers a search
* âœ… The system fetches the most relevant chunks (Top-K results)

  
  ğŸ“‚ **Goal:** Supply the LLM with context directly related to the question

---

### 3ï¸âƒ£ **Synthesis**

* âœ… The LLM processes retrieved context
* âœ… Generates a response based on the information
* âœ… Evaluated using RAG metrics

  
  ğŸ“‚ **Goal:** Ensure the response is accurate, relevant, and trustworthy

---

## ğŸ“Š **The RAG Triad: Core Evaluation Metrics**

To build a robust system, you need to measure how well itâ€™s performing. Thatâ€™s where the **RAG Triad** comes in.

---
<img width="962" height="996" alt="Screenshot 2025-09-13 at 4 34 40â€¯PM" src="https://github.com/user-attachments/assets/e04df145-7ceb-4210-b603-66636e8252ff" />


---

### ğŸ“Œ **Context Relevance**

Measures how closely the retrieved information aligns with the userâ€™s query

âœ” Helps identify retrieval issues

âœ” Ensures the LLM is getting useful data

---

### ğŸ“Œ **Answer Relevance**

Evaluates if the generated answer properly addresses the question

âœ” Reflects the quality of the LLMâ€™s reasoning

âœ” Ensures user satisfaction

---

### ğŸ“Œ **Groundedness**

Assesses whether the response is backed by the retrieved context

âœ” Reduces hallucinations

âœ” Builds trustworthiness in AI-generated responses

---

## ğŸš€ **Improving Retrieval & Evaluation Scores**

âœ… Optimize document chunking and embeddings during ingestion

âœ… Refine retrieval logic to boost context relevance

âœ… Ensure the LLM synthesizes grounded and relevant answers

âœ… Use the RAG triad for systematic, targeted improvements

---

## ğŸ“ˆ **Why This Matters**

* Streamlined workflows for question answering
* Enhanced accuracy and trust in AI responses
* Efficient experimentation and iteration
* Scalable, production-ready systems powered by metrics-driven insights

---
---

# ğŸ“š **Advanced Retrieval Techniques: Notes Summary**

Learn how retrieval strategies impact LLM performance by improving **context relevance (CR), groundedness(G), and answer relevance (AR)**.

---

## ğŸ” **1. Default Retrieval â€“ Baseline Approach**

â¡ **Definition:**
Topâ€‘K independent chunks retrieved by vector similarity, without neighboring sentences or hierarchy.
ğŸ“Š Corresponds to â€œDirect Query Engineâ€ in leaderboard.

â¡ **Observed Behavior:**

* Provides minimal context
* Lowest coherence and groundedness
* Example metrics:
  **CR:** 0.2550 | **G:** 0.80125 | **AR:** 0.930

â¡ **Takeaway:**
Fastest but fragmented context â†’ prone to hallucinations and weak grounding.

---

## ğŸŸ  **2. Sentenceâ€‘Window Retrieval â€“ Context Expansion**

---

<img width="968" height="994" alt="Screenshot 2025-09-13 at 4 40 13â€¯PM" src="https://github.com/user-attachments/assets/2479cb85-eb4c-4300-8129-79f6b1afd151" />

---

â¡ **Definition:**
After matching a sentence via embedding, retrieve it along with surrounding sentences to form a coherent passage.

â¡ **Observed Behavior:**

* Better context flow
* Higher relevance and grounding
* Example metrics:
  **CR:** 0.3675 | **G:** 0.8780 | **AR:** 0.925

â¡ **Takeaway:**
Improves local coherence by reducing boundary cuts.

---


## ğŸ”µ **3. Autoâ€‘Merging Retrieval â€“ Hierarchical Context**


---

<img width="960" height="1013" alt="Screenshot 2025-09-13 at 4 43 14â€¯PM" src="https://github.com/user-attachments/assets/b596496a-dedf-4b1a-b8f8-341dd649ba37" />

---
â¡ **Definition:**
Documents are structured as a tree; small â€œchildâ€ chunks are dynamically merged into larger â€œparentâ€ chunks when relevant.

â¡ **Observed Behavior:**

* Most coherent, complete context
* Balances detail and brevity
* Example metrics:
  **CR:** 0.4350 | **G:** 1.0000 | **AR:** 0.940

â¡ **Takeaway:**
Adaptive context sizing delivers the highest grounding with similar latency.

---

## ğŸ“ˆ **Why These Methods Work**

âœ… **More Complete Context:**
Neighboring sentences and parent nodes reduce fragmentation â†’ LLM sees a full semantic span.

âœ… **Adaptive Retrieval:**
Autoâ€‘merging adjusts context length dynamically â†’ supports better answer relevance.

âœ… **Measurable Improvement:**
Leaderboard example:

â¡ CR rises from **0.2550 â†’ 0.3675 â†’ 0.4350**

â¡ G rises from **0.80125 â†’ 0.8780 â†’ 1.0000**

---


## ğŸ“Š **Quick Comparison Table**

| ğŸ”¢ **Method**          | ğŸ“– **How it Works**                                   | ğŸ“Š **Example Metrics (G / AR / CR)** | âœ… **Effect**                                        |
| ---------------------- | ----------------------------------------------------- | ------------------------------------ | --------------------------------------------------- |
| âœ… **Default**          | Topâ€‘K chunks via embedding; no neighbors or hierarchy | 0.80125 / 0.930 / 0.2550             | Fast baseline, fragmented context                   |
| ğŸŸ  **Sentenceâ€‘window** | Retrieve hit sentence plus neighbors for coherence    | 0.8780 / 0.925 / 0.3675              | Better local context, improved grounding            |
| ğŸ”µ **Autoâ€‘merging**    | Tree of chunks; promote relevant child to parent      | 1.0000 / 0.940 / 0.4350              | Largest, most coherent context with similar latency |

---
<img width="962" height="645" alt="Screenshot 2025-09-13 at 4 43 51â€¯PM" src="https://github.com/user-attachments/assets/29b98804-4f0b-499d-af7a-1a5796068134" />

---
## ğŸ¯ **Infographic Takeaways**

* **Goal:** Improve Context Relevance, Groundedness, and Answer Relevance by enriching retrieved information.
* **Default Retrieval:** Quick but incomplete â†’ weak grounding.
* **Sentenceâ€‘Window:** Adds local neighbors â†’ smoother passages â†’ higher CR and G.
* **Autoâ€‘Merging:** Dynamically expands context â†’ full passages â†’ top performance.
* **Key Insight:** Thoughtful context enrichment drastically boosts AI trustworthiness and answer quality.

---


