---

# ğŸ“Š **Maximizing RAG Triad Scores â€“ Iterative Retrieval Strategy**

---

## âœ… **Goal**

â¡ï¸ Start with a simple retrieval baseline
â¡ï¸ Systematically vary sentence-window size
â¡ï¸ Track token cost vs relevance tradeoffs
â¡ï¸ Improve **context relevance**, **groundedness**, and **answer relevance**

> ğŸ’¬ *â€œIn interviews, I always stress starting simple. A well-defined baseline helps in isolating the real impact of any change, especially when dealing with complex models like RAG.â€*

---

## ğŸ§ª **Experiment Design**

ğŸ“‚ **Multiple Question Sets**
Run all app variants on the same evals â†’ ensure fair comparisons across experiments.

ğŸ“Š **Window Size Sweep**
Test sentence windows (e.g. 1, 3, 5) â†’ study how more context affects model responses.

ğŸ“ˆ **Use RAG Triad Metrics**
Measure context relevance, groundedness, and answer relevance â†’ maintain consistent evaluation.

> ğŸ’¬ *â€œWhenever I experiment, I treat evaluation as data-driven. Consistency in measurement avoids false positives and ensures improvements are reproducible.â€*

---

## ğŸ”„ **Iteration Loop**

1ï¸âƒ£ **Start Simple**
Use LlamaIndex basic retrieval â†’ find failure modes from limited context.

2ï¸âƒ£ **Introduce Sentence-Window Retrieval**
Augment context with neighboring sentences â†’ re-evaluate improvements.

3ï¸âƒ£ **Keep Experimenting**
Adjust window sizes â†’ identify configurations that maximize triad scores.

> ğŸ’¬ *â€œIteration is key â€” observing how changes in context affect downstream metrics helps us intelligently refine the approach rather than blindly tuning parameters.â€*

---

## ğŸ“‹ **What to Track**

ğŸ—‚ **Experiment Logs**
Record window size, question sets, scores, latency, token usage â†’ pick the best setup confidently.

âš– **Token vs Relevance Tradeoff**
Balance longer windows (higher cost) against gains in context relevance â†’ find the efficient frontier.

ğŸ” **Context vs Groundedness**
Track how improvements in context relevance often boost groundedness â†’ better support for answers.

> ğŸ’¬ *â€œI always recommend logging every run â€” even failures. They help trace performance trends and guide informed decisions in production pipelines.â€*

---

## ğŸ’¡ **Practical Tips**

â–¶ **Start Small**
Begin with window size 1 â†’ gradually increase â†’ avoid unnecessary cost early on.

ğŸ”„ **Compare After Every Change**
Re-run the triad metrics â†’ side-by-side comparison ensures improvements are real.

ğŸ“¦ **Use Fixed Datasets**
Keep the evaluation harness and question pool constant â†’ attribute differences to retrieval methods, not dataset drift.

> ğŸ’¬ *â€œStarting small and verifying changes systematically helps build scalable solutions without wasting compute or introducing noise into experiments.â€*

---

## âœ… **Key Takeaways**

* Iterative improvements rooted in data and metrics beat guesswork.
* Sentence-window retrieval can dramatically enhance context relevance and groundedness.
* Tracking token cost alongside accuracy helps optimize both performance and efficiency.
* Systematic experimentation with clear records leads to reliable, reproducible results.

> ğŸ’¬ *â€œIn interviews, I explain that optimization isnâ€™t about brute force â€” itâ€™s about structured experimentation and knowing where to invest effort for maximum impact.â€*

---

<img width="956" height="864" alt="1" src="https://github.com/user-attachments/assets/6745e01d-9678-4a87-a97c-97ba671ef9cc" />

---

<img width="958" height="728" alt="2" src="https://github.com/user-attachments/assets/e1f646bc-8cd8-4380-91cc-4da013c053f8" />

---

<img width="958" height="877" alt="3" src="https://github.com/user-attachments/assets/4201e4e3-e3d5-48e1-ab4c-489845196656" />

---
