---
<img width="959" height="883" alt="Screenshot 2025-09-14 at 1 01 21â€¯PM" src="https://github.com/user-attachments/assets/c94de7be-f3b8-488e-9b9d-0df4723701bb" />

---

---

<img width="963" height="996" alt="Screenshot 2025-09-14 at 1 01 43â€¯PM" src="https://github.com/user-attachments/assets/ca95bc77-8afb-4f74-b328-1d08c1d175e3" />

---

# ğŸ“š **Sentence-Window Retrieval â€“ Infographic Notes**

---

## âœ… **What is Sentence-Window Retrieval?**

ğŸ“– It expands a retrieved sentence by including neighboring sentences above and below, forming a coherent, self-contained passage.
ğŸ¯ Helps LLMs by preserving discourse, qualifiers, timelines, or definitions that would otherwise be lost in isolated fragments.

---

## âš™ **How It Works**

1. ğŸ” **Embedding Lookup:** Find Topâ€‘K sentences using standard embedding search.
2. â• **Add Local Context:** For each hit, attach adjacent sentences within a configurable window.
3. ğŸ“¦ **Feed to LLM:** Augmented chunks are passed to the LLM for synthesis without altering index structure.

---

## ğŸ“ˆ **Impact on RAG Metrics**

âœ” **Context Relevance:**
Nearby details enrich the topic match, aligning better with the query.

âœ” **Groundedness:**
Fuller passages supply clear evidence, reducing hallucinations.

âœ” **Answer Relevance:**
Contiguous context keeps the model on-topic for more accurate responses.

---

## ğŸ”§ **Tuning Tips**

* ğŸ“ **Window Size:**
  Start small â†’ expand until key qualifiers, definitions, or timelines are consistently included.

* âš– **Balance Topâ€‘K & Window:**
  Larger windows may require reducing Topâ€‘K to stay within token limits while preserving quality.

* ğŸ§© **Chunking Strategy:**
  Use smaller base chunks before windowing to avoid redundant information.

---

## âœ… **When to Prefer It**

âœ” Queries relying on qualifiers, caveats, or time-sensitive phrases
âœ” Multi-sentence facts or definitions spread across several lines
âœ” Cases where single-sentence retrieval strips away context necessary for reasoning

---

## ğŸ”„ **Quick Workflow**

1. ğŸ” Retrieve Topâ€‘K sentences using embeddings
2. â• Expand each with neighboring sentences
3. ğŸ“¦ Pass augmented contexts to the LLM
4. ğŸ“Š Score outputs using context relevance, groundedness, and answer relevance

---

## ğŸŸ  **Key Takeaways**

* Sentence-window retrieval enriches context without changing index structures.
* Itâ€™s ideal for queries requiring nuanced or multi-sentence understanding.
* Careful tuning of window size and chunk balance ensures coherent, on-topic outputs within token constraints.

